---
abstract: Dual-arm robotic grasping is crucial for handling large objects that require stable and coordinated manipulation. While single-arm grasping has been extensively studied, datasets tailored for dual-arm settings remain scarce. We introduce a large-scale dataset of 16 million dual-arm grasps, evaluated under improved force-closure constraints. Additionally, we develop a benchmark dataset containing 300 objects with approximately 30,000 grasps, evaluated in a physics simulation environment, providing a better grasp quality assessment for dual-arm grasp synthesis methods. Finally, we demonstrate the effectiveness of our dataset by training a Dual-Arm Grasp Classifier network that outperforms the state- of-the-art methods by 15%, achieving higher grasp success rates and improved generalization across objects.
authors:
- Mohammed Faizal Karim*
- admin*
- Shreya Bollimuntha
- Mahesh Reddy Tapeti
- Gaurav Singh
- Nagamanikandan Govindan
- K Madhava Krishna
date: "2025-03-07T00:00:00Z"
doi: ""
featured: true
image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/s9CC2SKySJM)'
  focal_point: ""
  preview_only: false
links:
- name: Custom Link
  url: http://example.org
projects:
- internal-project
publication: Submitted at IROS 2025
publication_short: ""
publication_types:
- article
# publishDate: "2024-09-21T00:00:00Z"
tags:
- Grasp Dataset Generation
title: DG16M - A Large-Scale Dataset for Dual-Arm Grasping with Force-Optimized Grasps
# url_code: https://github.com/HugoBlox/hugo-blox-builder
# url_dataset: '#'
# url_pdf: http://arxiv.org/pdf/1512.04133v1
# url_poster: '#'
url_project: "https://dg16m.github.io/DG-16M/"
# url_slides: ""
# url_source: '#'
url_video: 'https://drive.google.com/file/d/1vnQfPgby9De9SqwthkpZs1PkwcXJHdGG/view?usp=sharing'
---

<!-- This work is driven by the results in my [previous paper](/publication/conference-paper/) on LLMs. -->

<!-- {{% callout note %}}
Create your slides in Markdown - click the *Slides* button to check out the example.
{{% /callout %}} -->

Add the publication's **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/).
